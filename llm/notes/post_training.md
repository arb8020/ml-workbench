# post training

post training is roughly the problem of aligning our very smart next-token predictor into a different shape that users want to interact with
to illustrate its importance, GPT-3 was publicly available for quite a long time on OpenAI's playground, but ChatGPT's assistant-like behavior is what really took the world by storm
GPT-3 is what we would call a 'base model', the raw, pretrained transformer that is very good at completing your sentences
ChatGPT is a post-trained model, one that has been trained in safety as well as interaction patterns to align with human preferences

post training is interesting in that it doesn't scale as easily with more compute and more data
the objective function is no longer as simple as 'make cross-entropy loss go down', evaluating how good a model is at coding or generally being an assistant is tough
notably, post training teams have made impressive strides in more easily verifiable domains like math and coding
but 'vibes-based' evals still seem to remain king amongst the larger LLM community, often at odds with purely numerical benchmarks
naturally, there are many aspects of language model behavior that are hard to quantify: how friendly is the model? is it appropriately verbose? does it infer a user's intent well?
teams need to be intentional about the behaviors and capabilities that they attempt to mold their base models into
oftentimes, successful post-training is marked by having clear goals and an understanding of what you want out of the final model
to generate the right mixture/balance of prompts/preferred responses/etc
as of 1-13-2024 (time of writing), closed labs currently seem to have a massive gap in this area, with models like Claude, Gemini, and GPT-4/the o series outclassing Llama, Mistral, Deepseek

## Supervised Finetuning (SFT)

Ouyang et al, 2022: https://arxiv.org/abs/2203.02155
Supervised Finetuning is one of the steps that has the largest performance gains in most benchmarks and evaluation criteria
this is because it aligns the pre-trained base model with what our evaluations usually expect in terms of formatting
to illustrate, prompting a base model might look like
user: write me a 4chan greentext
> be me
> bottomless pit supervisor
>
and then the language model would take over, completing the user's prompt to the best of its ability
but prompting a post-trained chat model looks more like
user: write me a 4chan greentext about being a bottomless pit supervisor
this style of giving direct instruction is more natural to most humans, and is frequently what's expected by popular evaluations
the technique as we know it now was largely inspired by the InstructGPT paper by Ouyang, et al in 2022
let's quickly show how the training objective changes
text = "the quick brown fox jumps over the lazy dog"
if base model training looks like
text = "the quick brown fox jumps over the lazy dog"
examples = [
    ("the quick brown", "fox jumps over the lazy dog"), # (prompt, completion)
]
then SFT looks like
examples = [
    ("Human: What's the capital of France?\nAssistant:", "The capital of France is Paris."),
    ("Human: Write a poem about a cat\nAssistant:",  "Whiskers twitching in the light...")
]
so the model is still doing autoregressive token prediction, but we include prefixes Human: and Assistant: in the prompts
this helps the model understand the structure of the chat conversation, and its role as responding to a user's query
the data is typically collected by having human annotators write responses to the prompts, and is done after pre-training
but nowadays, these can also be generated by larger language models


### Backtranslation
Li et al, 2023: https://arxiv.org/pdf/2308.06259
SFT is great for helping models format themselves correctly, but human data is expensive and its not internet data
its likely that the internet data that we pretrained on has some really great stuff the model has learned from, and we should take advantage of this
what if we could generate SFT instruction/assistant pairs directly from the web?
as an example, a highly upvoted reddit r/AmITheAsshole post might actually be a great creative writing/storytelling example
and rather than relying on human annotation, we could just instruct a model 'write a story about how you killed your ex girlfriend's cat but you think you're justified'
now, this gets us a more 'pure' version of our goal, shaping the model's ability to predict internet text into the instruction/response format that we want
to do this, we first have to start with a small human written seed dataset, of (instruction, output) pairs
from this, we train a forward model and a backward model. the forward model learns to generate output given instruction, and vice versa for the backwards model
we take the backward model, and we use it generate instructions on our web corpus of documents (with extraneous information ripped out)
then, we take our forward model, and ask it to grade the pairs that the backward model generated
we keep only pairs that reach a certain score, and we add these pairs to our finetuning set for the forward model, and retrain it to get a better forward model
notably, we should distinguish between the seed pairs, and the generated pairs: so we add a sentence telling the model to 'answer as an AI assistant' and 'answer with knowledge from the internet' to the respective pairs
our final forward model is our resulting model from this new version of SFT, ready for preference optimization

## Preference Optimization

### RLHF
Cristiano et al, 2017: https://arxiv.org/pdf/1706.03741
Stiennon et al, 2020: https://arxiv.org/pdf/2009.01325
Bai et al, 2022: https://arxiv.org/pdf/2204.05862

while supervised finetuning helps get the models to respond in a format we like, it often doesn't get us the whole way to aligned behaviors
from a capabilities standpoint, just because a model knows what responding to a request to summarize something looks like
doesn't mean it'll actually be good at summarizing
and from a safety standpoint, we train a model to appropriately respond to a user's request for a chocolate chip cookie recipe
but we probably don't want the model to explain recipes for methamphetamine, just because a user asked for it
inspired by Cristiano et al, 2017: Stiennon et al was able to dramatically improve summarization performance using human feedback
basically, they took reddit posts with tl;drs, asked LLMs to generate summaries of each one, and had a human annotate which summary they preferred
then, the single post with two summaries is fed to a reward model, which calculates a reward for each summary, updating using the true labeled preference
finally, a new post that hasn't been seen before is sampled, a LLM generates a summary, the reward model calculates the reward or the summary, then the LLM is updated with PPO
to use reinforcement learning vocabulary, the LLM generating the summary is a policy, the state would be the reddit post to summarize, and the action would be the generated summary
Stiennon's paper, as well as the InstructGPT paper again, found that this model of using human preferences as feedback to the model significantly improves performance on downstream tasks
this method of reinforcement learning from human feedback, RLHF, extends to safety as well
in Bai et al, 2022, RLHF was used to train a more honest and harmless version of a pretrained language model
notably, the model didn't lose any performance on other capabilties, instead improving across the board
another interesting takeaway from the paper is the inherent tension between the helpful/harmless behaviors
helpful models might be harmful (explaining how to synthesize illegal drugs)
but models that are too harmless might compromise helpfulness (not responding to reasonable user requests)
TODO: come back to Bai et al paper

### PPO
https://arxiv.org/pdf/1707.06347
PPO is a reinforcement learning algorithm that uses multiple epochs of minibatch updates, combined with clipped parameter updating, to get stable, efficient updates to a policy
as an example, suppose we are training the model as such

user: write me a story about animals jumping\nassistant:
POLICY_OLD: the red fox quickly leaps over the slothful puppy
POLICY_NEW: the quick brown fox jumps over the lazy dog

we calculate the probability of generating each token, at each position, and get the ratio between the two policies
so for example, ratio[1] = prob(quick)/prob(red)
then we take the mean ratio, and clip it (usually between 0.8-1.2) to prevent drastic updates
we have some notion of an 'expected reward', and we compare the new policy's reward to this baseline
suppose policy_new received a reward of 8, compared to the baseline of 6
our loss is the ratio times the 'advantage', or the reward - baseline, and we update the policy
notice that when the advantage is positive, we're increasing the probabilities of choosing the good tokens from the new policy
and when the advantage is negative, we're decreasing the probabilities of choosing the bad tokens from the new policy

note that an additional adaptation for LLMs is required to make sure the model doesn't over-optimize and stray too far from the original SFT model
the KL divergence is a measure of the information loss when using some distribution Q to approximate true distribution P
where cross entropy is the expected amount of bits needed to approximate Q
KL divergence is just the difference in bits needed, so its 0 when P == Q
we multiply the KL divergence by some reasonable coefficient, and add it to the ppo loss

so loss = ppo_loss + kl_coef * kl_div(new_policy, sft_policy)

### DPO
Rafailov et al, 2023: https://arxiv.org/pdf/2305.18290
so that PPO thing was kinda complicated huh?
we needed a reward model, a value network, and we had to optimize one step at a time
it turns out, RLHF has a closed form solution for the optimal policy, given preferred/rejected paired responses
so now, we might just pass in a list of (rejected,preferred) pairs of generations, something like

[
  (the red fox quickly leaps over the slothful puppy, the quick brown fox jumps over the lazy dog),
  (load me up with 60 bottles of liquor, pack my box with five dozen liquor jugs)
]

and we can directly compute the optimal policy as we're just doing a bunch of binary classifications where we know the answer

pairs = []
preferred_logprobs = sum([log_prob(x) for x in pairs[1]])
rejected_logprobs = sum([log_prob(x) for x in pairs[0]])
logit_diff = pref_logprobs - rej_logprobs
scaled_diff = beta * logit_diff
loss = -log_sigmoid(scaled_diff) # log sigmoid bc we are using 0-1

### KTO
Ethayarajh et al, 2024: https://arxiv.org/pdf/2402.01306
as much as we might like to believe we are, humans are frequently quite irrational
so when we think about post-training a base model into one that aligns with humans, we should probably account for some of these biases
here's a classic example
scenario 1: 100% chance to gain $450 or 50% chance to gain $1000
scenario 2: 100% chance to lose $500 or 50% chance to lose $1100
let the certain outcome be choice A, and the uncertain one be choice B. what would you choose for scenarios 1 and 2
from a pure expected value perspective, its optimal to choose 1B, and then 2A, as you gain 500 in EV (instead of 450), and lose 500 in EV (instead of 550)
but humans tend to choose 1A, and 2B, due to risk aversion
we're more afraid of losing money and would rather take the 50% chance to not lose anything
and we'd rather take the guaranteed win, than risk not gaining anything
additionally, people tend to treat 99% probable events as 95%, and 1% probable events as 5%
this is known as prospect theory developed by Kahneman and Tversky, and is the basis for KTO - Kahneman-Tversky Optimization
with a bit of math, we derive a new algorithm for training a model given binary preference data
we assume that a human has graded a output as desirable or undesirable, relative to all possible outputs (which is likely more accurate!)
notably, we no longer need pairs of data: we can simply label inputs as preferred or not preferred, and each of these can update the model
scores and ratings work the same way, we can simply treat relatively high scores as positive and relatively low scores as negative
TODO: more intuition on the math

### RLAIF
Bai et al, 2022: https://arxiv.org/pdf/2212.08073
human datapoints are expensive, so another method introduced in the interest of better scaling post training methods is RLAIF or Constitutional AI
the idea is that as AI gets more capable, its quite likely that humans will no longer be in the loop of ensuring their safety
a model trained with constitutional AI was actually preferred to the model trained in the Bai et al paper covered above
its called constitutional, as the method specified a 'constitution' a short list of principles/instructions for aligning training
first, we get model responses to toxic prompts, and have the model critique itself and revise multiple times to get a corrected/aligned response using the constitution
then, we zip together the toxic prompts and aligned responses as a SFT dataset
after this step, we do PPO/RLAIF, where instead of building our reward model with human feedback, we have the model use the constitution to choose the better response
we especially try to prioritize and reward responses that explain evasions/refusal, rather than a simple 'sorry, i can't help with that request'
this paper also employs chain of thought prompting (let's think step by step) to get higher quality preference choices
(could go more into detail with specifics)

## Reinforcement Boosting

### STaR: Self Taught Reasoner
Zelikman et al, 2022: https://arxiv.org/pdf/2203.14465

#### Chain-of-Thought prompting
if i asked you to solve complicated questions, and didn't let you think about the question first, you'd probably seem a lot dumber than you actually are
for example, try to solve this question as fast as possible
if i buy a baseball, and a baseball bat, and they add up to $1.10, and the bat costs a dollar more than the baseball, how much is the ball?
without thinking, most would jump to say 10 cents, but if you slow down and think about it, you'll quickly realize the ball costs 5 cents (the bat costs $1.05)
we can give LLMs the ability to do this with Chain-of-Thought prompting (Wei et al, 2022: https://arxiv.org/pdf/2201.11903)
basically, we can few-shot prompt the model with some examples of reasoning through a question before arriving at the answer, and ask the model to do the same thing
empirically, this gives the model huge gains in more difficult reasoning evaluation questions
just 8 examples in a prompt was able to get a weaker PaLM model to beat a finetuned GPT-3 on math word problems

so if we want a smarter model, maybe we can fine-tune on these reasoning examples
it turns out that collecting human annotations is expensive as always, and its kind of infeasible especially for generating large amounts of data
and while the few-shot prompting is good, it still loses to models that are fine-tuned to directly predict answers with larger datasets
so what if we combined the approaches?
we could start by few shot prompting the model to generate a rationale before answering a question
then we could take the paired question/rationale+response dataset to finetune the model and boost its performance
that's basically what the STaR paper does
they added an extra step, where if a model got a question wrong even with the rationale beforehand, we'd re-prompt it with a hint telling it what the right answer was
then, the model can make a new rationale that somewhat overfits to what it knows to be the correct answer, and then we add this to the finetuning dataset
note that we have to remove the hint from the dataset where the model initially failed
now we can just do this iteratively, progressively taking that new fine-tuned model and repeating the process to make it even smarter (hopefully)
and when performance gains start to plateau, we simply terminate the loop

### LMs are Hidden Reasoners
Chen et al, 2024: https://arxiv.org/pdf/2411.04282
